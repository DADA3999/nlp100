{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80. ID番号への変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "categories = ['b', 't', 'e', 'm']\n",
    "category_names = ['business', 'science and technology', 'entertainment', 'health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x = re.sub(r'\\s+', ' ', x)\n",
    "    x = nlp.make_doc(x)\n",
    "    x = [d.text for d in x]\n",
    "    return x\n",
    "\n",
    "def read_feature_dataset(filename):\n",
    "    with open(filename) as f:\n",
    "        dataset = f.read().splitlines()\n",
    "    dataset = [line.split('\\t') for line in dataset]\n",
    "    dataset_t = [categories.index(line[0]) for line in dataset]\n",
    "    dataset_x = [tokenize(line[1]) for line in dataset]\n",
    "    return dataset_x, torch.tensor(dataset_t, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_t = read_feature_dataset('data/train.txt')\n",
    "valid_x, valid_t = read_feature_dataset('data/valid.txt')\n",
    "test_x, test_t = read_feature_dataset('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9700"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter([\n",
    "    x\n",
    "    for sent in train_x\n",
    "    for x in sent\n",
    "])\n",
    "\n",
    "vocab_in_train = [\n",
    "    token\n",
    "    for token, freq in counter.most_common()\n",
    "    if freq > 1\n",
    "]\n",
    "len(vocab_in_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語彙を用意し，ID番号の列に変換できるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = ['[UNK]'] + vocab_in_train\n",
    "vocab_dict = {x:n for n, x in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_ids(sent):\n",
    "    return torch.tensor([vocab_dict[x if x in vocab_dict else '[UNK]'] for x in sent], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kathleen', 'Sebelius', \"'\", 'LGBT', 'legacy']\n",
      "tensor([   0,    0,    2, 2648,    0])\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(sent_to_ids(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID番号の列に変換しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_ids(dataset):\n",
    "    return [sent_to_ids(x) for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s = dataset_to_ids(train_x)\n",
    "valid_s = dataset_to_ids(valid_x)\n",
    "test_s = dataset_to_ids(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   0,    0,    2, 2648,    0]),\n",
       " tensor([   9, 6740, 1445, 2076,  583,   10,  547,   32,   51,  873, 6741]),\n",
       " tensor([   0,  205, 4198,  315, 1899, 1232,    0])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence as pad\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの長さが可変長になったので，データセットのクラスも少し変わります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, source, target):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.lengths = torch.tensor([len(x) for x in source])\n",
    "        self.size = len(source)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'src':self.source[index],\n",
    "            'trg':self.target[index],\n",
    "            'lengths':self.lengths[index],\n",
    "        }\n",
    "    \n",
    "    def collate(self, xs):\n",
    "        return {\n",
    "            'src':pad([x['src'] for x in xs]),\n",
    "            'trg':torch.stack([x['trg'] for x in xs], dim=-1),\n",
    "            'lengths':torch.stack([x['lengths'] for x in xs], dim=-1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset, width, shuffle = False):\n",
    "        self.dataset = dataset\n",
    "        self.width = width\n",
    "        self.shuffle = shuffle\n",
    "        if not shuffle:\n",
    "            self.indices = torch.arange(len(dataset))\n",
    "            \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = torch.randperm(len(self.dataset))\n",
    "        index = 0\n",
    "        while index < len(self.dataset):\n",
    "            yield self.indices[index : index + self.width]\n",
    "            index += self.width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ内の系列の長さが降順になってるとうれしい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescendingSampler(Sampler):\n",
    "    def __init__(self, dataset, width, shuffle = False):\n",
    "        assert not shuffle\n",
    "        super().__init__(dataset, width, shuffle)\n",
    "        self.indices = self.indices[self.dataset.lengths[self.indices].argsort(descending=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batchを事例数で積んでいくと無駄なパディングが多く発生するので，なるべく同じ長さのシーケンスをバッチに積んで，バッチのトークン数に基づいてバッチを切っていきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxTokensSampler(Sampler):\n",
    "    def __iter__(self):\n",
    "        self.indices = torch.randperm(len(self.dataset))\n",
    "        self.indices = self.indices[self.dataset.lengths[self.indices].argsort(descending=True)]\n",
    "        for batch in self.generate_batches():\n",
    "            yield batch\n",
    "\n",
    "    def generate_batches(self):\n",
    "        batches = []\n",
    "        batch = []\n",
    "        acc = 0\n",
    "        max_len = 0\n",
    "        for index in self.indices:\n",
    "            acc += 1\n",
    "            this_len = self.dataset.lengths[index]\n",
    "            max_len = max(max_len, this_len)\n",
    "            if acc * max_len > self.width:\n",
    "                batches.append(batch)\n",
    "                batch = [index]\n",
    "                acc = 1\n",
    "                max_len = this_len\n",
    "            else:\n",
    "                batch.append(index)\n",
    "        if batch != []:\n",
    "            batches.append(batch)\n",
    "        rd.shuffle(batches)\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loader(dataset, width, sampler=Sampler, shuffle=False, num_workers=8):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_sampler = sampler(dataset, width, shuffle),\n",
    "        collate_fn = dataset.collate,\n",
    "        num_workers = num_workers,\n",
    "    )\n",
    "\n",
    "def gen_descending_loader(dataset, width, num_workers=0):\n",
    "    return gen_loader(dataset, width, sampler = DescendingSampler, shuffle = False, num_workers = num_workers)\n",
    "\n",
    "def gen_maxtokens_loader(dataset, width, num_workers=0):\n",
    "    return gen_loader(dataset, width, sampler = MaxTokensSampler, shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを用意する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_s, train_t)\n",
    "valid_dataset = Dataset(valid_s, valid_t)\n",
    "test_dataset = Dataset(test_s, test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMのモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, v_size, e_size, h_size, c_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(v_size, e_size)\n",
    "        self.rnn = nn.LSTM(e_size, h_size, num_layers = 1)\n",
    "        self.out = nn.Linear(h_size, c_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight' in name or 'bias' in name:\n",
    "                param.data.uniform_(-0.1, 0.1)\n",
    "        self.out.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def forward(self, batch, h=None):\n",
    "        x = self.embed(batch['src'])\n",
    "        x = pack(x, batch['lengths'])\n",
    "        x, (h, c) = self.rnn(x, h)\n",
    "        h = self.out(h)\n",
    "        return h.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(len(vocab_dict), 300, 50, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測する(問題文にあるsoftmaxはかけてない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = gen_loader(test_dataset, 10, DescendingSampler, False)\n",
    "model(iter(loader).next()).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TaskとTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self):\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def train_step(self, model, batch):\n",
    "        model.zero_grad()\n",
    "        loss = self.criterion(model(batch), batch['trg'])\n",
    "        loss.backward()\n",
    "        return loss.item()\n",
    "    \n",
    "    def valid_step(self, model, batch):\n",
    "        with torch.no_grad():\n",
    "            loss = self.criterion(model(batch), batch['trg'])\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loaders, task, optimizer, max_iter, device = None):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.train_loader, self.valid_loader = loaders\n",
    "        self.task = task\n",
    "        self.optimizer = optimizer\n",
    "        self.max_iter = max_iter\n",
    "        self.device = device\n",
    "    \n",
    "    def send(self, batch):\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(self.device)\n",
    "        return batch\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        acc = 0\n",
    "        for n, batch in enumerate(self.train_loader):\n",
    "            batch = self.send(batch)\n",
    "            acc += self.task.train_step(self.model, batch)\n",
    "            self.optimizer.step()\n",
    "        return acc / n\n",
    "            \n",
    "    def valid_epoch(self):\n",
    "        self.model.eval()\n",
    "        acc = 0\n",
    "        for n, batch in enumerate(self.valid_loader):\n",
    "            batch = self.send(batch)\n",
    "            acc += self.task.valid_step(self.model, batch)\n",
    "        return acc / n\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.max_iter):\n",
    "            train_loss = self.train_epoch()\n",
    "            valid_loss = self.valid_epoch()\n",
    "            print('epoch {}, train_loss:{:.5f}, valid_loss:{:.5f}'.format(epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUはね，使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.14506, valid_loss:1.73364\n",
      "epoch 1, train_loss:0.96299, valid_loss:1.11744\n",
      "epoch 2, train_loss:0.67876, valid_loss:0.85982\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model = LSTMClassifier(len(vocab_dict), 300, 128, 4)\n",
    "loaders = (\n",
    "    gen_loader(train_dataset, 1),\n",
    "    gen_loader(valid_dataset, 1),\n",
    ")\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 3, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測もする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model, loader, device=None):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.device = device\n",
    "        \n",
    "    def send(self, batch):\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(self.device)\n",
    "        return batch\n",
    "        \n",
    "    def infer(self, batch):\n",
    "        self.model.eval()\n",
    "        batch = self.send(batch)\n",
    "        return self.model(batch).argmax(dim=-1).item()\n",
    "        \n",
    "    def predict(self):\n",
    "        lst = []\n",
    "        for batch in self.loader:\n",
    "            lst.append(self.infer(batch))\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true, pred):\n",
    "    return np.mean([t == p for t, p in zip(true, pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.7611381505054287\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.6511976047904192\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.23482, valid_loss:1.31786\n",
      "epoch 1, train_loss:1.12792, valid_loss:1.20727\n",
      "epoch 2, train_loss:1.06968, valid_loss:1.16238\n",
      "epoch 3, train_loss:0.93787, valid_loss:0.94840\n",
      "epoch 4, train_loss:0.86169, valid_loss:1.09106\n",
      "epoch 5, train_loss:0.81993, valid_loss:0.86653\n",
      "epoch 6, train_loss:0.66291, valid_loss:0.77162\n",
      "epoch 7, train_loss:0.71991, valid_loss:0.82280\n",
      "epoch 8, train_loss:0.53705, valid_loss:0.77397\n",
      "epoch 9, train_loss:0.46455, valid_loss:0.72747\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(len(vocab_dict), 300, 128, 4)\n",
    "loaders = (\n",
    "    gen_maxtokens_loader(train_dataset, 4000),\n",
    "    gen_descending_loader(valid_dataset, 128),\n",
    ")\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.2, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.8475290153500562\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.7769461077844312\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embed(embed):\n",
    "    for i, token in enumerate(vocab_list):\n",
    "        if token in vectors:\n",
    "            embed.weight.data[i] = torch.from_numpy(vectors[token])\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.22026, valid_loss:1.26922\n",
      "epoch 1, train_loss:0.98941, valid_loss:0.84883\n",
      "epoch 2, train_loss:0.61283, valid_loss:0.68030\n",
      "epoch 3, train_loss:0.51964, valid_loss:0.63204\n",
      "epoch 4, train_loss:0.47092, valid_loss:0.61985\n",
      "epoch 5, train_loss:0.44848, valid_loss:0.59426\n",
      "epoch 6, train_loss:0.40102, valid_loss:0.53780\n",
      "epoch 7, train_loss:0.36047, valid_loss:0.48343\n",
      "epoch 8, train_loss:0.32368, valid_loss:0.45806\n",
      "epoch 9, train_loss:0.29477, valid_loss:0.41828\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(len(vocab_dict), 300, 128, 4)\n",
    "init_embed(model.embed)\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.9108947959565705\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.875748502994012\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, v_size, e_size, h_size, c_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(v_size, e_size)\n",
    "        self.rnn = nn.LSTM(e_size, h_size, num_layers = 2, bidirectional = True)\n",
    "        self.out = nn.Linear(h_size * 2, c_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.uniform_(self.embed.weight, -0.1, 0.1)\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight' in name or 'bias' in name:\n",
    "                nn.init.uniform_(param, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.out.weight, -0.1, 0.1)\n",
    "    \n",
    "    def forward(self, batch, h=None):\n",
    "        x = self.embed(batch['src'])\n",
    "        x = pack(x, batch['lengths'])\n",
    "        x, (h, c) = self.rnn(x, h)\n",
    "        h = h[-2:]\n",
    "        h = h.transpose(0,1)\n",
    "        h = h.contiguous().view(-1, h.size(1) * h.size(2))\n",
    "        h = self.out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.20451, valid_loss:1.33967\n",
      "epoch 1, train_loss:0.95726, valid_loss:0.78972\n",
      "epoch 2, train_loss:0.58861, valid_loss:0.66202\n",
      "epoch 3, train_loss:0.52411, valid_loss:0.63551\n",
      "epoch 4, train_loss:0.48568, valid_loss:0.61630\n",
      "epoch 5, train_loss:0.45090, valid_loss:0.61073\n",
      "epoch 6, train_loss:0.47044, valid_loss:0.57788\n",
      "epoch 7, train_loss:0.40242, valid_loss:0.63106\n",
      "epoch 8, train_loss:0.37824, valid_loss:0.53034\n",
      "epoch 9, train_loss:0.32058, valid_loss:0.50757\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMClassifier(len(vocab_dict), 300, 128, 4)\n",
    "init_embed(model.embed)\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.9036877573942343\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.8637724550898204\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PADのあるデータセットにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_vocab_list = ['[PAD]', '[UNK]'] + vocab_in_train\n",
    "cnn_vocab_dict = {x:n for n, x in enumerate(cnn_vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kathleen', 'Sebelius', \"'\", 'LGBT', 'legacy']\n",
      "tensor([   1,    1,    3, 2649,    1])\n"
     ]
    }
   ],
   "source": [
    "def cnn_sent_to_ids(sent):\n",
    "    return torch.tensor([cnn_vocab_dict[x if x in cnn_vocab_dict else '[UNK]'] for x in sent], dtype=torch.long)\n",
    "\n",
    "print(train_x[0])\n",
    "print(cnn_sent_to_ids(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_dataset_to_ids(dataset):\n",
    "    return [cnn_sent_to_ids(x) for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_s = cnn_dataset_to_ids(train_x)\n",
    "cnn_valid_s = cnn_dataset_to_ids(valid_x)\n",
    "cnn_test_s = cnn_dataset_to_ids(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   1,    1,    3, 2649,    1]),\n",
       " tensor([  10, 6741, 1446, 2077,  584,   11,  548,   33,   52,  874, 6742]),\n",
       " tensor([   1,  206, 4199,  316, 1900, 1233,    1])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_train_s[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasetのパディングはpadを使わないことにしました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDataset(Dataset):\n",
    "    def collate(self, xs):\n",
    "        max_seq_len = max([x['lengths'] for x in xs])\n",
    "        src = [torch.cat([x['src'], torch.zeros(max_seq_len - x['lengths'], dtype=torch.long)], dim=-1) for x in xs]\n",
    "        src = torch.stack(src)\n",
    "        mask = [[1] * x['lengths'] + [0] * (max_seq_len - x['lengths']) for x in xs]\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        return {\n",
    "            'src':src,\n",
    "            'trg':torch.tensor([x['trg'] for x in xs]),\n",
    "            'mask':mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_dataset = CNNDataset(cnn_train_s, train_t)\n",
    "cnn_valid_dataset = CNNDataset(cnn_valid_s, valid_t)\n",
    "cnn_test_dataset = CNNDataset(cnn_test_s, test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNモデルをつくっていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, v_size, e_size, h_size, c_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(v_size, e_size)\n",
    "        self.conv = nn.Conv1d(e_size, h_size, 3, padding=1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(h_size, c_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.normal_(self.embed.weight, 0, 0.1)\n",
    "        nn.init.kaiming_normal_(self.conv.weight)\n",
    "        nn.init.constant_(self.conv.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.out.weight)\n",
    "        nn.init.constant_(self.out.bias, 0)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = self.embed(batch['src'])\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x.transpose(-1, -2))\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x.masked_fill_(batch['mask'].unsqueeze(-2) == 0, -1e4)\n",
    "        x = F.max_pool1d(x, x.size(-1)).squeeze(-1)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87. 確率的勾配降下法によるCNNの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn_embed(embed):\n",
    "    for i, token in enumerate(cnn_vocab_list):\n",
    "        if token in vectors:\n",
    "            embed.weight.data[i] = torch.from_numpy(vectors[token])\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.09698, valid_loss:0.87693\n",
      "epoch 1, train_loss:0.69122, valid_loss:0.70549\n",
      "epoch 2, train_loss:0.56959, valid_loss:0.62338\n",
      "epoch 3, train_loss:0.49819, valid_loss:0.57069\n",
      "epoch 4, train_loss:0.45295, valid_loss:0.52958\n",
      "epoch 5, train_loss:0.41466, valid_loss:0.49520\n",
      "epoch 6, train_loss:0.38196, valid_loss:0.46547\n",
      "epoch 7, train_loss:0.36166, valid_loss:0.44768\n",
      "epoch 8, train_loss:0.34073, valid_loss:0.43178\n",
      "epoch 9, train_loss:0.32250, valid_loss:0.41370\n"
     ]
    }
   ],
   "source": [
    "model = CNNClassifier(len(cnn_vocab_dict), 300, 128, 4)\n",
    "init_cnn_embed(model.embed)\n",
    "loaders = (\n",
    "    gen_maxtokens_loader(cnn_train_dataset, 4000),\n",
    "    gen_descending_loader(cnn_valid_dataset, 32),\n",
    ")\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.9080868588543617\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(cnn_train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データでの正解率 : 0.8839820359281437\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(cnn_test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88. パラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCNNDataset(Dataset):\n",
    "    def collate(self, xs):\n",
    "        max_seq_len = max([x['lengths'] for x in xs])\n",
    "        mask = [[1] * x['lengths'] + [0] * (max_seq_len - x['lengths']) for x in xs]\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        return {\n",
    "            'src':pad([x['src'] for x in xs]),\n",
    "            'trg':torch.stack([x['trg'] for x in xs], dim=-1),\n",
    "            'mask':mask,\n",
    "            'lengths':torch.stack([x['lengths'] for x in xs], dim=-1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnncnn_train_dataset = BiLSTMCNNDataset(cnn_train_s, train_t)\n",
    "rnncnn_valid_dataset = BiLSTMCNNDataset(cnn_valid_s, valid_t)\n",
    "rnncnn_test_dataset = BiLSTMCNNDataset(cnn_test_s, test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCNNClassifier(nn.Module):\n",
    "    def __init__(self, v_size, e_size, h_size, c_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(v_size, e_size)\n",
    "        self.rnn = nn.LSTM(e_size, h_size, bidirectional = True)\n",
    "        self.conv = nn.Conv1d(h_size* 2, h_size, 3, padding=1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(h_size, c_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.uniform_(self.embed.weight, -0.1, 0.1)\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight' in name or 'bias' in name:\n",
    "                nn.init.uniform_(param, -0.1, 0.1)\n",
    "        nn.init.kaiming_normal_(self.conv.weight)\n",
    "        nn.init.constant_(self.conv.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.out.weight)\n",
    "        nn.init.constant_(self.out.bias, 0)\n",
    "    \n",
    "    def forward(self, batch, h=None):\n",
    "        x = self.embed(batch['src'])\n",
    "        x = self.dropout(x)\n",
    "        x = pack(x, batch['lengths'])\n",
    "        x, (h, c) = self.rnn(x, h)\n",
    "        x, _ = unpack(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x.permute(1, 2, 0))\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x.masked_fill_(batch['mask'].unsqueeze(-2) == 0, -1)\n",
    "        x = F.max_pool1d(x, x.size(-1)).squeeze(-1)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.15425, valid_loss:1.03456\n",
      "epoch 1, train_loss:0.77422, valid_loss:0.71770\n",
      "epoch 2, train_loss:0.57652, valid_loss:0.62928\n",
      "epoch 3, train_loss:0.51651, valid_loss:0.59283\n",
      "epoch 4, train_loss:0.48866, valid_loss:0.56999\n",
      "epoch 5, train_loss:0.46092, valid_loss:0.55249\n",
      "epoch 6, train_loss:0.43789, valid_loss:0.53115\n",
      "epoch 7, train_loss:0.41153, valid_loss:0.50594\n",
      "epoch 8, train_loss:0.38908, valid_loss:0.48521\n",
      "epoch 9, train_loss:0.36271, valid_loss:0.45827\n",
      "評価データでの正解率 : 0.8675149700598802\n",
      "epoch 0, train_loss:1.12504, valid_loss:1.01977\n",
      "epoch 1, train_loss:0.75765, valid_loss:0.69227\n",
      "epoch 2, train_loss:0.55254, valid_loss:0.60689\n",
      "epoch 3, train_loss:0.49968, valid_loss:0.58016\n",
      "epoch 4, train_loss:0.46818, valid_loss:0.54989\n",
      "epoch 5, train_loss:0.43520, valid_loss:0.51396\n",
      "epoch 6, train_loss:0.40337, valid_loss:0.48612\n",
      "epoch 7, train_loss:0.36728, valid_loss:0.44508\n",
      "epoch 8, train_loss:0.33284, valid_loss:0.42158\n",
      "epoch 9, train_loss:0.31212, valid_loss:0.39170\n",
      "評価データでの正解率 : 0.8892215568862275\n",
      "epoch 0, train_loss:1.16690, valid_loss:1.02263\n",
      "epoch 1, train_loss:0.75951, valid_loss:0.70778\n",
      "epoch 2, train_loss:0.56198, valid_loss:0.61136\n",
      "epoch 3, train_loss:0.50309, valid_loss:0.57744\n",
      "epoch 4, train_loss:0.46121, valid_loss:0.54518\n",
      "epoch 5, train_loss:0.42452, valid_loss:0.50533\n",
      "epoch 6, train_loss:0.39402, valid_loss:0.46894\n",
      "epoch 7, train_loss:0.35871, valid_loss:0.44116\n",
      "epoch 8, train_loss:0.32932, valid_loss:0.41125\n",
      "epoch 9, train_loss:0.31069, valid_loss:0.39188\n",
      "評価データでの正解率 : 0.8832335329341318\n",
      "epoch 0, train_loss:1.00516, valid_loss:0.86773\n",
      "epoch 1, train_loss:0.60939, valid_loss:0.62852\n",
      "epoch 2, train_loss:0.50880, valid_loss:0.56735\n",
      "epoch 3, train_loss:0.45502, valid_loss:0.52573\n",
      "epoch 4, train_loss:0.40882, valid_loss:0.48002\n",
      "epoch 5, train_loss:0.36222, valid_loss:0.44183\n",
      "epoch 6, train_loss:0.33247, valid_loss:0.40807\n",
      "epoch 7, train_loss:0.30624, valid_loss:0.38527\n",
      "epoch 8, train_loss:0.28908, valid_loss:0.37882\n",
      "epoch 9, train_loss:0.27282, valid_loss:0.36792\n",
      "評価データでの正解率 : 0.8929640718562875\n",
      "epoch 0, train_loss:0.95130, valid_loss:0.75686\n",
      "epoch 1, train_loss:0.54003, valid_loss:0.57349\n",
      "epoch 2, train_loss:0.44523, valid_loss:0.51922\n",
      "epoch 3, train_loss:0.38315, valid_loss:0.44421\n",
      "epoch 4, train_loss:0.33938, valid_loss:0.41322\n",
      "epoch 5, train_loss:0.31023, valid_loss:0.38644\n",
      "epoch 6, train_loss:0.28560, valid_loss:0.36845\n",
      "epoch 7, train_loss:0.27365, valid_loss:0.36483\n",
      "epoch 8, train_loss:0.26139, valid_loss:0.36701\n",
      "epoch 9, train_loss:0.24756, valid_loss:0.35496\n",
      "評価データでの正解率 : 0.8937125748502994\n"
     ]
    }
   ],
   "source": [
    "loaders = (\n",
    "    gen_maxtokens_loader(rnncnn_train_dataset, 4000),\n",
    "    gen_descending_loader(rnncnn_valid_dataset, 32),\n",
    ")\n",
    "task = Task()\n",
    "for h in [32, 64, 128, 256, 512]:\n",
    "    model = BiLSTMCNNClassifier(len(cnn_vocab_dict), 300, h, 4)\n",
    "    init_cnn_embed(model.embed)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9, nesterov=True)\n",
    "    trainer = Trainer(model, loaders, task, optimizer, 10, device)\n",
    "    trainer.train()\n",
    "    predictor = Predictor(model, gen_loader(rnncnn_test_dataset, 1), device)\n",
    "    pred = predictor.predict()\n",
    "    print('評価データでの正解率 :', accuracy(test_t, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89. 事前学習済み言語モデルからの転移学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface/transformers( https://github.com/huggingface/transformers )を使っていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_for_bert(filename):\n",
    "    with open(filename) as f:\n",
    "        dataset = f.read().splitlines()\n",
    "    dataset = [line.split('\\t') for line in dataset]\n",
    "    dataset_t = [categories.index(line[0]) for line in dataset]\n",
    "    dataset_x = [torch.tensor(tokenizer.encode(line[1]), dtype=torch.long) for line in dataset]\n",
    "    return dataset_x, torch.tensor(dataset_t, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_x, bert_train_t = read_for_bert('data/train.txt')\n",
    "bert_valid_x, bert_valid_t = read_for_bert('data/valid.txt')\n",
    "bert_test_x, bert_test_t = read_for_bert('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def collate(self, xs):\n",
    "        max_seq_len = max([x['lengths'] for x in xs])\n",
    "        src = [torch.cat([x['src'], torch.zeros(max_seq_len - x['lengths'], dtype=torch.long)], dim=-1) for x in xs]\n",
    "        src = torch.stack(src)\n",
    "        mask = [[1] * x['lengths'] + [0] * (max_seq_len - x['lengths']) for x in xs]\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        return {\n",
    "            'src':src,\n",
    "            'trg':torch.tensor([x['trg'] for x in xs]),\n",
    "            'mask':mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_dataset = BertDataset(bert_train_x, bert_train_t)\n",
    "bert_valid_dataset = BertDataset(bert_valid_x, bert_valid_t)\n",
    "bert_test_dataset = BertDataset(bert_test_x, bert_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_pretrained('bert-base-cased', num_labels=4)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-cased', config=config)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = self.bert(batch['src'], attention_mask=batch['mask'])\n",
    "        return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:0.60020, valid_loss:0.35551\n",
      "epoch 1, train_loss:0.25203, valid_loss:0.27956\n",
      "epoch 2, train_loss:0.15712, valid_loss:0.26447\n",
      "epoch 3, train_loss:0.10070, valid_loss:0.29965\n",
      "epoch 4, train_loss:0.07287, valid_loss:0.30565\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier()\n",
    "loaders = (\n",
    "    gen_maxtokens_loader(bert_train_dataset, 1000),\n",
    "    gen_descending_loader(bert_valid_dataset, 32),\n",
    ")\n",
    "task = Task()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 5, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.9904530138524897\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(bert_train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(train_t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データでの正解率 : 0.9281437125748503\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(bert_test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('学習データでの正解率 :', accuracy(test_t, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
